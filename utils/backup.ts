import { getDatabase, initDatabase } from "./database.ts";
import { getConfig } from "./env.ts";
import { ensureDir } from "https://deno.land/std@0.216.0/fs/mod.ts";
import { join } from "https://deno.land/std@0.216.0/path/mod.ts";

// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç¨®é¡
export type BackupType = "full" | "incremental" | "schema";

// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
export interface BackupMetadata {
  id: string;
  type: BackupType;
  filename: string;
  createdAt: string;
  size: number;
  checksum: string;
  description?: string;
  tables: string[];
  recordCount: number;
  compressed: boolean;
}

// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çµæœ
export interface BackupResult {
  success: boolean;
  metadata?: BackupMetadata;
  error?: string;
  duration: number;
}

// ãƒªã‚¹ãƒˆã‚¢çµæœ
export interface RestoreResult {
  success: boolean;
  restoredTables: string[];
  restoredRecords: number;
  error?: string;
  duration: number;
}

// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
export interface BackupConfig {
  backupDir: string;
  maxBackups: number;
  retentionDays: number;
  compression: boolean;
  encryption: boolean;
  excludeTables: string[];
}

// ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã®è¨ˆç®—
async function calculateChecksum(data: Uint8Array): Promise<string> {
  const hashBuffer = await crypto.subtle.digest("SHA-256", data);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
}

// ãƒ‡ãƒ¼ã‚¿ã®åœ§ç¸®
async function compressData(data: Uint8Array): Promise<Uint8Array> {
  const stream = new CompressionStream("gzip");
  const writer = stream.writable.getWriter();
  const reader = stream.readable.getReader();
  
  const chunks: Uint8Array[] = [];
  
  // åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å–ã‚Š
  const readPromise = (async () => {
    let done = false;
    while (!done) {
      const { value, done: readerDone } = await reader.read();
      done = readerDone;
      if (value) {
        chunks.push(value);
      }
    }
  })();
  
  // ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿
  await writer.write(data);
  await writer.close();
  
  await readPromise;
  
  // åœ§ç¸®ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
  const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const chunk of chunks) {
    result.set(chunk, offset);
    offset += chunk.length;
  }
  
  return result;
}

// ãƒ‡ãƒ¼ã‚¿ã®è§£å‡
async function decompressData(compressedData: Uint8Array): Promise<Uint8Array> {
  const stream = new DecompressionStream("gzip");
  const writer = stream.writable.getWriter();
  const reader = stream.readable.getReader();
  
  const chunks: Uint8Array[] = [];
  
  // è§£å‡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å–ã‚Š
  const readPromise = (async () => {
    let done = false;
    while (!done) {
      const { value, done: readerDone } = await reader.read();
      done = readerDone;
      if (value) {
        chunks.push(value);
      }
    }
  })();
  
  // ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿
  await writer.write(compressedData);
  await writer.close();
  
  await readPromise;
  
  // è§£å‡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
  const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const chunk of chunks) {
    result.set(chunk, offset);
    offset += chunk.length;
  }
  
  return result;
}

// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒ©ã‚¹
export class DatabaseBackup {
  private config: BackupConfig;
  
  constructor(config?: Partial<BackupConfig>) {
    const appConfig = getConfig();
    
    this.config = {
      backupDir: "./backups",
      maxBackups: 30,
      retentionDays: appConfig.backupRetentionDays,
      compression: true,
      encryption: false,
      excludeTables: ["error_logs"], // å¤§ãããªã‚ŠãŒã¡ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é™¤å¤–
      ...config,
    };
  }
  
  // ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
  async createFullBackup(description?: string): Promise<BackupResult> {
    const startTime = performance.now();
    
    try {
      await initDatabase();
      const db = getDatabase();
      
      // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
      await ensureDir(this.config.backupDir);
      
      // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—IDã®ç”Ÿæˆ
      const backupId = `full_${Date.now()}`;
      const timestamp = new Date().toISOString();
      
      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚­ãƒ¼ãƒã¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
      const dumpData = await this.dumpDatabase(db, "full");
      
      // ãƒ‡ãƒ¼ã‚¿ã®åœ§ç¸®
      let finalData = new TextEncoder().encode(dumpData.sql);
      if (this.config.compression) {
        finalData = await compressData(finalData);
      }
      
      // ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã®è¨ˆç®—
      const checksum = await calculateChecksum(finalData);
      
      // ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
      const extension = this.config.compression ? ".sql.gz" : ".sql";
      const filename = `${backupId}${extension}`;
      const filepath = join(this.config.backupDir, filename);
      
      // ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
      await Deno.writeFile(filepath, finalData);
      
      // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
      const metadata: BackupMetadata = {
        id: backupId,
        type: "full",
        filename,
        createdAt: timestamp,
        size: finalData.length,
        checksum,
        description,
        tables: dumpData.tables,
        recordCount: dumpData.recordCount,
        compressed: this.config.compression,
      };
      
      // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
      await this.saveMetadata(metadata);
      
      const duration = performance.now() - startTime;
      
      console.log(`âœ… Full backup created: ${filename} (${this.formatSize(finalData.length)}, ${Math.round(duration)}ms)`);
      
      return {
        success: true,
        metadata,
        duration,
      };
      
    } catch (error) {
      const duration = performance.now() - startTime;
      console.error("âŒ Backup failed:", error);
      
      return {
        success: false,
        error: error.message,
        duration,
      };
    }
  }
  
  // ã‚¹ã‚­ãƒ¼ãƒã®ã¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
  async createSchemaBackup(description?: string): Promise<BackupResult> {
    const startTime = performance.now();
    
    try {
      await initDatabase();
      const db = getDatabase();
      
      await ensureDir(this.config.backupDir);
      
      const backupId = `schema_${Date.now()}`;
      const timestamp = new Date().toISOString();
      
      // ã‚¹ã‚­ãƒ¼ãƒã®ã¿ã‚’å–å¾—
      const dumpData = await this.dumpDatabase(db, "schema");
      
      let finalData = new TextEncoder().encode(dumpData.sql);
      if (this.config.compression) {
        finalData = await compressData(finalData);
      }
      
      const checksum = await calculateChecksum(finalData);
      const extension = this.config.compression ? ".schema.sql.gz" : ".schema.sql";
      const filename = `${backupId}${extension}`;
      const filepath = join(this.config.backupDir, filename);
      
      await Deno.writeFile(filepath, finalData);
      
      const metadata: BackupMetadata = {
        id: backupId,
        type: "schema",
        filename,
        createdAt: timestamp,
        size: finalData.length,
        checksum,
        description,
        tables: dumpData.tables,
        recordCount: 0,
        compressed: this.config.compression,
      };
      
      await this.saveMetadata(metadata);
      
      const duration = performance.now() - startTime;
      
      console.log(`âœ… Schema backup created: ${filename} (${Math.round(duration)}ms)`);
      
      return {
        success: true,
        metadata,
        duration,
      };
      
    } catch (error) {
      const duration = performance.now() - startTime;
      console.error("âŒ Schema backup failed:", error);
      
      return {
        success: false,
        error: error.message,
        duration,
      };
    }
  }
  
  // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ€ãƒ³ãƒ—
  private async dumpDatabase(db: any, type: BackupType): Promise<{
    sql: string;
    tables: string[];
    recordCount: number;
  }> {
    const sqlStatements: string[] = [];
    const tables: string[] = [];
    let totalRecordCount = 0;
    
    // SQLiteã®ãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—
    const tableQuery = `
      SELECT name, sql FROM sqlite_master 
      WHERE type='table' AND name NOT LIKE 'sqlite_%'
      ORDER BY name
    `;
    
    const allTables = db.prepare(tableQuery).all() as Array<{name: string, sql: string}>;
    
    for (const table of allTables) {
      if (this.config.excludeTables.includes(table.name)) {
        continue;
      }
      
      tables.push(table.name);
      
      // ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ–‡ã‚’è¿½åŠ 
      sqlStatements.push(`-- Table: ${table.name}`);
      sqlStatements.push(`DROP TABLE IF EXISTS ${table.name};`);
      sqlStatements.push(`${table.sql};`);
      sqlStatements.push("");
      
      // ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ãƒ³ãƒ—ï¼ˆã‚¹ã‚­ãƒ¼ãƒã®ã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
      if (type !== "schema") {
        const records = db.prepare(`SELECT * FROM ${table.name}`).all();
        totalRecordCount += records.length;
        
        if (records.length > 0) {
          // ã‚«ãƒ©ãƒ åã‚’å–å¾—
          const columns = Object.keys(records[0]);
          const columnNames = columns.join(", ");
          
          sqlStatements.push(`-- Data for table: ${table.name}`);
          
          // ãƒãƒƒãƒã§INSERTæ–‡ã‚’ç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
          const batchSize = 100;
          for (let i = 0; i < records.length; i += batchSize) {
            const batch = records.slice(i, i + batchSize);
            const values = batch.map(record => {
              const valueList = columns.map(col => {
                const value = record[col];
                if (value === null) return "NULL";
                if (typeof value === "string") return `'${value.replace(/'/g, "''")}'`;
                return String(value);
              }).join(", ");
              return `(${valueList})`;
            }).join(",\n  ");
            
            sqlStatements.push(`INSERT INTO ${table.name} (${columnNames}) VALUES`);
            sqlStatements.push(`  ${values};`);
          }
          sqlStatements.push("");
        }
      }
    }
    
    // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¿½åŠ 
    const indexQuery = `
      SELECT sql FROM sqlite_master 
      WHERE type='index' AND name NOT LIKE 'sqlite_%' AND sql IS NOT NULL
      ORDER BY name
    `;
    
    const indexes = db.prepare(indexQuery).all() as Array<{sql: string}>;
    
    if (indexes.length > 0) {
      sqlStatements.push("-- Indexes");
      for (const index of indexes) {
        sqlStatements.push(`${index.sql};`);
      }
      sqlStatements.push("");
    }
    
    // ãƒ˜ãƒƒãƒ€ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
    const header = [
      `-- Nightlife Navigator Database Backup`,
      `-- Type: ${type}`,
      `-- Created: ${new Date().toISOString()}`,
      `-- Tables: ${tables.length}`,
      `-- Records: ${totalRecordCount}`,
      `-- Generated by Nightlife Navigator Backup System`,
      "",
      "PRAGMA foreign_keys=OFF;",
      "BEGIN TRANSACTION;",
      "",
    ];
    
    const footer = [
      "",
      "COMMIT;",
      "PRAGMA foreign_keys=ON;",
    ];
    
    const fullSql = [...header, ...sqlStatements, ...footer].join("\n");
    
    return {
      sql: fullSql,
      tables,
      recordCount: totalRecordCount,
    };
  }
  
  // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
  private async saveMetadata(metadata: BackupMetadata): Promise<void> {
    const metadataFile = join(this.config.backupDir, `${metadata.id}.meta.json`);
    await Deno.writeTextFile(metadataFile, JSON.stringify(metadata, null, 2));
  }
  
  // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§ã®å–å¾—
  async listBackups(): Promise<BackupMetadata[]> {
    try {
      const backups: BackupMetadata[] = [];
      
      for await (const entry of Deno.readDir(this.config.backupDir)) {
        if (entry.name.endsWith(".meta.json")) {
          try {
            const metadataPath = join(this.config.backupDir, entry.name);
            const metadataContent = await Deno.readTextFile(metadataPath);
            const metadata: BackupMetadata = JSON.parse(metadataContent);
            backups.push(metadata);
          } catch (error) {
            console.warn(`Failed to read metadata file: ${entry.name}`, error);
          }
        }
      }
      
      // ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
      return backups.sort((a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime());
      
    } catch (error) {
      if (error instanceof Deno.errors.NotFound) {
        return [];
      }
      throw error;
    }
  }
  
  // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒ
  async restoreFromBackup(backupId: string): Promise<RestoreResult> {
    const startTime = performance.now();
    
    try {
      const backups = await this.listBackups();
      const backup = backups.find(b => b.id === backupId);
      
      if (!backup) {
        throw new Error(`Backup not found: ${backupId}`);
      }
      
      const backupPath = join(this.config.backupDir, backup.filename);
      
      // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
      let sqlData = await Deno.readFile(backupPath);
      
      // åœ§ç¸®ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è§£å‡
      if (backup.compressed) {
        sqlData = await decompressData(sqlData);
      }
      
      const sqlContent = new TextDecoder().decode(sqlData);
      
      // ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã®æ¤œè¨¼
      const currentChecksum = await calculateChecksum(
        backup.compressed ? await compressData(new TextEncoder().encode(sqlContent)) : sqlData
      );
      
      if (currentChecksum !== backup.checksum) {
        throw new Error("Backup file checksum mismatch - file may be corrupted");
      }
      
      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å¾©å…ƒ
      await initDatabase();
      const db = getDatabase();
      
      // ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å†…ã§å¾©å…ƒã‚’å®Ÿè¡Œ
      db.exec("BEGIN TRANSACTION;");
      
      try {
        // SQLã‚’å®Ÿè¡Œ
        db.exec(sqlContent);
        db.exec("COMMIT;");
        
        const duration = performance.now() - startTime;
        
        console.log(`âœ… Database restored from backup: ${backup.filename} (${Math.round(duration)}ms)`);
        
        return {
          success: true,
          restoredTables: backup.tables,
          restoredRecords: backup.recordCount,
          duration,
        };
        
      } catch (sqlError) {
        db.exec("ROLLBACK;");
        throw sqlError;
      }
      
    } catch (error) {
      const duration = performance.now() - startTime;
      console.error("âŒ Restore failed:", error);
      
      return {
        success: false,
        restoredTables: [],
        restoredRecords: 0,
        error: error.message,
        duration,
      };
    }
  }
  
  // å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤
  async cleanupOldBackups(): Promise<{
    deletedCount: number;
    freedSpace: number;
  }> {
    try {
      const backups = await this.listBackups();
      const now = new Date();
      const cutoffDate = new Date(now.getTime() - this.config.retentionDays * 24 * 60 * 60 * 1000);
      
      let deletedCount = 0;
      let freedSpace = 0;
      
      // ä¿æŒæœŸé–“ã‚’éããŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
      for (const backup of backups) {
        const backupDate = new Date(backup.createdAt);
        if (backupDate < cutoffDate) {
          try {
            // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            const backupPath = join(this.config.backupDir, backup.filename);
            await Deno.remove(backupPath);
            freedSpace += backup.size;
            
            // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            const metadataPath = join(this.config.backupDir, `${backup.id}.meta.json`);
            await Deno.remove(metadataPath);
            
            deletedCount++;
            console.log(`ğŸ—‘ï¸  Deleted old backup: ${backup.filename}`);
            
          } catch (error) {
            console.warn(`Failed to delete backup: ${backup.filename}`, error);
          }
        }
      }
      
      // æœ€å¤§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°ã®åˆ¶é™
      const remainingBackups = backups.filter(b => new Date(b.createdAt) >= cutoffDate);
      if (remainingBackups.length > this.config.maxBackups) {
        const excessBackups = remainingBackups
          .sort((a, b) => new Date(a.createdAt).getTime() - new Date(b.createdAt).getTime())
          .slice(0, remainingBackups.length - this.config.maxBackups);
        
        for (const backup of excessBackups) {
          try {
            const backupPath = join(this.config.backupDir, backup.filename);
            await Deno.remove(backupPath);
            freedSpace += backup.size;
            
            const metadataPath = join(this.config.backupDir, `${backup.id}.meta.json`);
            await Deno.remove(metadataPath);
            
            deletedCount++;
            console.log(`ğŸ—‘ï¸  Deleted excess backup: ${backup.filename}`);
            
          } catch (error) {
            console.warn(`Failed to delete backup: ${backup.filename}`, error);
          }
        }
      }
      
      if (deletedCount > 0) {
        console.log(`âœ… Cleanup completed: ${deletedCount} backups deleted, ${this.formatSize(freedSpace)} freed`);
      }
      
      return { deletedCount, freedSpace };
      
    } catch (error) {
      console.error("âŒ Cleanup failed:", error);
      return { deletedCount: 0, freedSpace: 0 };
    }
  }
  
  // ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
  private formatSize(bytes: number): string {
    const units = ['B', 'KB', 'MB', 'GB'];
    let size = bytes;
    let unitIndex = 0;
    
    while (size >= 1024 && unitIndex < units.length - 1) {
      size /= 1024;
      unitIndex++;
    }
    
    return `${size.toFixed(1)}${units[unitIndex]}`;
  }
}

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
export const defaultBackup = new DatabaseBackup();